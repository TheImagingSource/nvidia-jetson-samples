{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MIPI Sensors with GStreamer\n",
    "\n",
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the GStreamer module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gi\n",
    "gi.require_version(\"Gst\", \"1.0\")\n",
    "from gi.repository import Gst\n",
    "\n",
    "Gst.init(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GStreamer works with graphs of media-handling components, called \"Pipelines\". Components could be:\n",
    "\n",
    " - *source*-elements that create media samples\n",
    " - *intermediate*-elements that process, transform, analyze or route the samples\n",
    " - *sink*-elements that finally consumes the samples\n",
    " \n",
    "The *source* and *sink* elements are mandatory.\n",
    "\n",
    "The source element for camera modules connected via the CSI-2 port (\"MIPI modules\") is called **nvarguscamerasrc**. Let'\n",
    "s create a simple pipeline that just produces and consumes data from the camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Gst.parse_launch(\"nvarguscamerasrc name=src ! appsink max-buffers=1 name=sink\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid black; background-color:#e3ffb3; padding:8px; margin-top: auto;\">\n",
    "    <strong><i>Tip:</i></strong>\n",
    "The <code>max-buffers</code> property should always be set on the appsink because it would otherwise store an unlimited amount of buffers if they are not consumed by the application. This will cause the system to go out-of-memory sooner or later.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could start the pipeline. This will set up the camera and the source element will start producing media samples (ie.: images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.set_state(Gst.State.PLAYING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`set_state` returned `GST_STATE_CHANGE_ASYNC` which means that the whole process of setting up the camera and starting the video stream is now running in the background while our python code continues. Let's make sure, everything is ready before we continue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_state(Gst.CLOCK_TIME_NONE)[0] == Gst.StateChangeReturn.SUCCESS and \"OK\" or \"FAILED!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our simple pipeline, we can extract the image data from the *appsink* element. This can be done by activating the \"try-pull-sample\" action. It gets a timeout value as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sink = pipeline.get_by_name(\"sink\")\n",
    "\n",
    "sample = sink.emit(\"try-pull-sample\", 1 * Gst.SECOND)\n",
    "print(\"Caps:\", sample.get_caps().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The appsink element returns a `GstSample` object. This object contains metadata like the `GstCaps` object that we just printed. It also contains the actual buffer which holds the image data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = sample.get_buffer()\n",
    "print(\"Got a buffer of size: %d Bytes\" % (buffer.get_size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the buffer size is only a few bytes. Having a look at the `GstCaps` again it says that the video format is: \n",
    "\n",
    "> video/x-raw(**memory:NVMM**), width=(int)1920, height=(int)1080\n",
    "\n",
    "*NVMM* memory is accessible from the GPU subsystem of the Tegra SoC. This allows for fast processing of the data using the GPU, ISP or video encoder hardware on the SoC. On the downside, this memory is not directly accessible from Python. So we could pass this buffer on to other GStreamer components that can work with NVMM memory but the Python script itself can not access the data. \n",
    "\n",
    "For now, we stop the pipeline to release the camera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.set_state(Gst.State.NULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
